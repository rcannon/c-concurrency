
- put thread_files in /scratch and move once program is finished.
- use calloc
- ALL THIS IS ASSUMING WE STORE THINGS IN ROW_MAJOR ORDER
- row wise mvp only writes any individual value of result vector once
    - col wise writes value n_cols times.
- takes longer to write var than get variable
    - has to ensure cache coherency
- only write contiguously, but read randomly is okay
- repeat tasks you want to time until total time is at least 5 min
- dont use malloc because 
    1. it might not give you a space that is big enough,
        and once you fill in data it might copy things over to larger space
    2. malloc will succeed even when you ask for more memory than you have access to.
- calloc takes two sizes:
    1. number of members
    2. size of each member
    - tell is one member and calcuate total space yourself
        - so it allocates memory in a single contiguous block
- dont allocate in loops
    - causes memory leaks
    - potential to allocate more memory than available, so program fails
        - thus one need to check to see if using more mem than available
        - so in worst case, just allocate as much as available at beginning
- freeing memory is operating system function
    - when your code terminates, the system will free the memory you allocated
        - OS will recalaim virtual address space
        - all pages that were used will be marked as not in use (also marked dirty)
        - UNLESS IT IS SHARED (be sure to do shmdt() at end of program)
- compiler cant track dataflow after about 120 lines
    - because code optimization is intractible in large programs
- timing:
    - time1 = setup_time + n_interations * time_per_iter
    - time2 = setup_time + (2 * n_iterations) * time per_iter
    - setuptime = 2*time1 - time2
    - n_interations * time_per_iter = time2 - time1
- parallel row wise mvp reduces the number of communications that need to be done (over prallel col-wise),
    since you don't need to share colummns
- only do block-row (not block-column) inner product
- block algorithm requires access to fewer elements of vector (more data reuse)
    - save on memory accesses (much slower than cpu operations)
    - this is especially true for matrix-matrix products
- gpus are really good at 4x4 dense matrix operations (because colors in video games
- print is system call - this is why we save_errno
- touched files hshould be in /tmp or /scratch
    - copy large input files there as well
- row wise inner product should be best
- thread 0 should build matrix,
    then share it in separate shm segment
- cant have clients write their result to same vector, because overlapping cache lines
- try not using all the cores, just 63 or 62
    - offset binds so none bind to cpu 0
    - bind to my_thread_id +1 or +2
- profile on intel compiler: pass -p flag when compiling and linking
    - when running, it will create output file that you can pass to gprof
- assemble resulting vector into shared memory.
    - client sends result to server, which puts it in shared segment.
        - there are faster ways, but when it comes to distributed memory this is best.
    - shared segment because in real problems the resulting vector will probably need to be used again.
- be sure to msync different updates separately in order to ensure updates
    are flushed to memory in correct order
- thread 0 makes shared mem segment for matrix (and another for vector, and another for result) AFTER all threads have attached to comm shared segment
    - pass the shmids to the threads using the communicating shared segment
- instead of having server do aggregation, have even number threads get result from odd numbered threads 
    - or set up communication tree (these are much more complicated, need to figure out communication between workers)
    - this will be more important for distributed computing
    - for starters, just have server rank do aggregation
- have server do work?
    - with large enough/ complicated/ multistep problems, better to just have server manage
- scheduling:
    - block scheduling : each worker i does rows i*(n/p) to (i+1)n/p (usually the best for symmetric tasks)
    - round-robin : each worker does every p-th row, starting at i (good for load balancing when tasks are sorted by size, or adjacent tasks are similar in some way)
    - dynamic : hand out next row when worker is done (better for sparse matrices, distributed memory)
- operations to avoid:
    - integer multiply : most computers now just send integers to floating point multiply and convert
    - mod : better to do integer dived, then multiply and subtract from original
        - especially don't do mod powers of 2 : better to just subtract one AND with 1 (bitwise op)
- plus/minus 2-3 seconds on matrix operations is expected (on about 5 min total run time)
- merge sort is best for HPC
    - data accesses are consecutive (both read and write)
    - however (duh), you must be able to store whole list in memory 
    - avoid quicksort (worst case n^2, lost of data movement, no-consecutive memory access)
    - use bin sort for sorting column indices of sparse matrix
- c flag -ftrapuv
    - set uninitialized stack space to unusual value to help with error detection
    - DONT USE ON PRODUCTION CODE - slows it down because takes time to initialize
- use calloc instead of malloc
    - because malloc is lazy
        - it doesn't check that space is available
        - waits until you need memory to allocate it
    - calloc - allocated it and initializes it to zero
        - does take a little time
        - memory allocation errors occur immediately
- use memset to set block of memory to particular value
    - man page : $ man3 memset
    - man2/man3 always gives programming interface
        - man give command line interface (not necessarily the same)
- ieee7554 float point format:
    - need to know standard
    - most significant bit is sign bit
    - mantissa : 
        - normalize exponent so left most bit is one (so you don't have to store it)
        - don't store leading zeros, just normalize until leading bit is one.
- when you add things with different exponenets, exponents have to get aligned
    - because mantissas are normalized, bit have to be shifted
        - so bits might be shifted off end
    - adding large numbers to small number results in loss of accuracy
        - this is especially bad when adding LOTS of small numbers to big number
    - cancellation errors occur when you add numbers that are close but opposite sign
- only useful thing about strassen matrix mult.:
    - hierarchically managing submatrices (just like blocking)
- adds are necessarily less accurate than multiplies
- rounding policy : prefer even number
- compiler directives:
    - give compiler extra info so it can optimize better
- server tasks management (for resiliency)
    - three lists:
        - unscheduled task list
        - sheduled task list
        - completed task list
    - once server has run out of completed task:
        - resend uncompleted tasks to finished clients 
            - in case some clients encountered hardware error
- why intel compiler:
    - fewer register spills
        - aka when data must be moved out of register to make room for other data because that other data wasnt added at right time.
- look into xavx2
- Horner's rule for polynomial evaluation
    - fuzed multiply adds
- threads should allocate local memory after they are forked,
    so the memory allocated is close to them (i.e. in computers with 2 physical cpu chips)


Idea: server knows which blocks client is working on by thread_id
    - client works on while row of blocks
    - give client every pth row of blocks? or just block of blocks?
    - define global constant block size
    - per_thread_mem_size = num_elements_in_block * sizeof(double) + server_struct size + client_struct_size
        - OR better, set up second shared segment for each client's result
        - client_struct should have number of how many block elements in result are filled
            - for when block size doesn't evenly divide matrix_size
        - block sizes wont be too big (defeats purpose of parallelism), so wont take long to flush




Sparse Matrices:
- usually > 99% zeros
- store nonzero elements of spare matrix as collection of triples { (row, col, A[row, col]) }
    - important how you order elements in list
    - want column indices to be close together, gnerally
- fma = fused multiply, add operation: y <- ax + b
- inner product of row i with vector b:
    - loop over nonzero elements row i, multiply element at col j with correspondin element in b
- compressed row storage format: 
    - let nzr be __ of triples of nonzero elements, assume matrix has n rows
    - three vectors: 
        - row index pointer (length n+1)
        - column index pointer of length nzr
        - value pointer of length nzr
    - keep all elements in single row adjacent in memory




Dense matrix Task:
- generate benchmark program
- inputs: size of matrix, vector to be multiplied, number of mvp's to compute, check accuracy yes/no
- if reading stuff from file, make sure to add comment line to parser
- benchmark program
    - ingest parameters
    - use calloc to allocate matrix
    - check that pointer is not null! (error and exit if so)
    - build matrix (square)
    - build the vector
    - if (check_accuracy)
        - build exact answer
        - of one mvp
        - build test routine that will compare computed answer with exact answer
    - loop n_iterations
        - do mvp

Matrix structure (1D array):
    - each element in diagonal = n_rows (= n_cols)
    - j > i (above diag) -> 1
    - i > j -> -1

Vector structure: v[i] = i for i = 0..(n-1)

Result: res[i]  = n * i + \sum_{j = i+1}^n j - \sum_{j = 1}^{i -1} j
                = n(n-1)/2 - i(i+1)/2 - i(i-1)/2 + n*i
                = n(n-1)/2 - i(n-i)


// row major, inner product
double* matrix;
double* row;
double* vector;
double* result;
row = matrix
for (i =0; i < n, i++){
    result[i] = dot(n, row, vector);
    row += n; // address arith
}
double
dot(n,row, vec)
{
    double res;
    for (j = 0; j < n; j++){
        res = res + row[i] * vec[i];
    }
    return res;
}

// row major, middle product
double* matrix;
double* col
double* a_ij
col = matrix;
for (i =0; i< n; i++) { res[i] = 0.0; } // calloc does this, but if you are doing multiple them it might be a good idea
for (j = 0; j < n; j++){ // loop over cols
    a_ij = col;
    for (i = 0; i < n; i++) { // loop over rows
        res[i] = res[i] + vec[j]* (*a_ij)
        a_ij = a_ij + n; // address arith
    }
    col = col + 1;
}

// block mvp method

